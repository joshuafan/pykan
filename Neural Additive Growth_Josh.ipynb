{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Neural Additive Growths?\n",
    "\n",
    "Investigatory notebook for playing around with different ideas for learning additivity in neural networks (or possibly Kolmogorov-Arnorld Networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from entmax import sparsemax, entmax15, entmax_bisect, normmax_bisect, budget_bisect\n",
    "\n",
    "from torcheval.metrics.functional import mean_squared_error\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map x, y coordinates to a set of features z, just so we can visualize things easier\n",
    "def xy_to_z(x, y):\n",
    "    return torch.cat([torch.sin(2*np.pi*x), torch.sin(2*np.pi*y), torch.cos(2*np.pi*x), \n",
    "                      torch.cos(2*np.pi*y), x+y, -x*y, torch.exp(x), torch.exp(y), torch.pow(x+y, 2), torch.pow(x-y, 2),\n",
    "                      -torch.pow(x+y, 2), torch.pow(y-x, 2)], dim=1)\n",
    "\n",
    "def plot_data(xy, z, ms=20, title=\"\"):\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    scatter = ax.scatter(xy[:,0].cpu().detach().numpy(), xy[:,1].cpu().detach().numpy(), c=z.cpu().detach().numpy(), cmap='viridis', s=ms)\n",
    "    colorbar = fig.colorbar(scatter)\n",
    "\n",
    "    ax.set_title(title)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "xy = torch.rand((15000, 2)).to(device)\n",
    "\n",
    "z = xy_to_z(xy[:,0].unsqueeze(1), xy[:,1].unsqueeze(1))\n",
    "\n",
    "num_features=12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_features):\n",
    "    plot_data(xy, z[:,i], title=\"z_{}\".format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "We then generate the remaining data from a random set of functions of each of the 6 variables in a relatively simple/naive process, which we call $g(z)$:\n",
    "\n",
    "1. For each variable, select a random function from among $\\sin(2*\\pi*z_i), \\cos(2*\\pi*z_i), \\exp(z_i), \\sqrt{|z_i|})$\n",
    "2. For each (disjoint) pair of variables (i.e. $(z_0, z_1), (z_2, z_3), (z_4, z_5)$), select a random function from among $z_i*z_j, \\sin(2*\\pi*(z_i+z_j)), \\cos(2*\\pi*(z_i+z_j))$\n",
    "3. Randomly generate a bias value $\\beta \\in [a, b]$ fpr some $a, b$\n",
    "4. Add the resulting values together with some noise $\\epsilon$ to generate a final value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleFunction:\n",
    "    def __init__(self, i):\n",
    "        self.i = i\n",
    "\n",
    "    def calculate(self, z):\n",
    "        raise Exception(\"Abstract Class should not be called\")\n",
    "\n",
    "    def equation(self):\n",
    "        raise Exception(\"Abstract Class should not be called\")\n",
    "\n",
    "class Exp(SingleFunction):\n",
    "    def __init__(self, i):\n",
    "        super().__init__(i)\n",
    "\n",
    "    def calculate(self, z):\n",
    "        return torch.exp(z[:,self.i])\n",
    "\n",
    "    def equation(self):\n",
    "        return \"e^z{}\".format(self.i)\n",
    "\n",
    "class Sqrt(SingleFunction):\n",
    "    def __init__(self, i):\n",
    "        super().__init__(i)\n",
    "\n",
    "    def calculate(self, z):\n",
    "        return torch.sqrt(torch.abs(z[:,self.i]))\n",
    "\n",
    "    def equation(self):\n",
    "        return \"z{}^(1/2)\".format(self.i)\n",
    "\n",
    "class Sin(SingleFunction):\n",
    "    def __init__(self, i):\n",
    "        super().__init__(i)\n",
    "\n",
    "    def calculate(self, z):\n",
    "        return torch.sin(2*np.pi*z[:,self.i])\n",
    "\n",
    "    def equation(self):\n",
    "        return \"sin(2*pi*z{})\".format(self.i)\n",
    "\n",
    "class Cos(SingleFunction):\n",
    "    def __init__(self, i):\n",
    "        super().__init__(i)\n",
    "\n",
    "    def calculate(self, z):\n",
    "        return torch.cos(2*np.pi*z[:,self.i])\n",
    "\n",
    "    def equation(self):\n",
    "        return \"cos(2*pi*z{})\".format(self.i)\n",
    "\n",
    "single_funcs = [Exp, Sqrt, Sin, Cos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultFunction:\n",
    "    def __init__(self, i, j):\n",
    "        self.i = i\n",
    "        self.j = j\n",
    "\n",
    "    def calculate(self, z):\n",
    "        raise Exception(\"Abstract Class should not be called\")\n",
    "\n",
    "    def equation(self):\n",
    "        raise Exception(\"Abstract Class should not be called\")\n",
    "\n",
    "class Cos2(MultFunction):\n",
    "    def __init__(self, i, j):\n",
    "        super().__init__(i, j)\n",
    "\n",
    "    def calculate(self, z):\n",
    "        return torch.cos(2*np.pi*(z[:,self.i] + z[:,self.j]))\n",
    "\n",
    "    def equation(self):\n",
    "        return \"cos(2*pi*(z{} + z{}))\".format(self.i, self.j)\n",
    "\n",
    "class Sin2(MultFunction):\n",
    "    def __init__(self, i, j):\n",
    "        super().__init__(i, j)\n",
    "\n",
    "    def calculate(self, z):\n",
    "        return torch.sin(2*np.pi*(z[:,self.i] + z[:,self.j]))\n",
    "\n",
    "    def equation(self):\n",
    "        return \"sin(2*pi*(z{} + z{}))\".format(self.i, self.j)\n",
    "\n",
    "class Mult(MultFunction):\n",
    "    def __init__(self, i, j):\n",
    "        super().__init__(i, j)\n",
    "\n",
    "    def calculate(self, z):\n",
    "        return z[:,self.i] * z[:,self.j]\n",
    "\n",
    "    def equation(self):\n",
    "        return \"z{}*z{}\".format(self.i, self.j)\n",
    "\n",
    "multi_funcs = [Cos2, Sin2, Mult]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "    def __init__(self, rng, a=0, b=10, noise_str=0.2, num_features=12):\n",
    "        self.bias = rng.random() * (b - a) + a\n",
    "        self.eqs = [\"{:.3f}\".format(self.bias)]\n",
    "\n",
    "        self.noise_str = noise_str\n",
    "\n",
    "        # Add single variable functions\n",
    "        self.f1s = []\n",
    "        for i in range(num_features):\n",
    "            func = rng.choice(single_funcs)(i)\n",
    "        \n",
    "            self.f1s.append(func)\n",
    "            self.eqs.append(func.equation())\n",
    "\n",
    "        # Add two variable functions\n",
    "        self.f2s = []\n",
    "        for i in range(0, num_features, 2):\n",
    "            func = rng.choice(multi_funcs)(i, i+1)\n",
    "\n",
    "            self.f2s.append(func)\n",
    "            self.eqs.append(func.equation())\n",
    "\n",
    "    def equation(self):\n",
    "        return \" + \".join(self.eqs) + \" + eps\"\n",
    "\n",
    "    def calculate(self, z):\n",
    "        v = torch.ones(z.shape[0]).to(device) * self.bias\n",
    "\n",
    "        # Add single variable functions\n",
    "        for f1 in self.f1s:\n",
    "            v += f1.calculate(z)\n",
    "\n",
    "        # Add two variable functions\n",
    "        for f2 in self.f2s:\n",
    "            v += f2.calculate(z)\n",
    "\n",
    "        noises = torch.randn_like(v) * self.noise_str\n",
    "\n",
    "        v += noises\n",
    "        \n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=1234)\n",
    "\n",
    "data_generator = DataGenerator(rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"labels = {}\".format(data_generator.equation()))\n",
    "\n",
    "labels = data_generator.calculate(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(xy, labels, title=\"True Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, z, labels, device):\n",
    "        self.z = torch.Tensor(z).to(device)\n",
    "        self.labels = torch.Tensor(labels).unsqueeze(1).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        z = self.z[idx]\n",
    "        labels = self.labels[idx]\n",
    "\n",
    "        return z, labels\n",
    "\n",
    "z_train = z[:10000]\n",
    "labels_train = labels[:10000]\n",
    "\n",
    "z_valid = z[10000:12500]\n",
    "labels_valid = labels[10000:12500]\n",
    "\n",
    "z_test = z[12500:]\n",
    "labels_test = labels[12500:]\n",
    "\n",
    "train_ds = Dataset(z_train, labels_train, device)\n",
    "valid_ds = Dataset(z_valid, labels_valid, device)\n",
    "test_ds = Dataset(z_test, labels_test, device)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, shuffle=True, batch_size=64)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_ds, batch_size=64)\n",
    "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "Model constructions for the synthetic data\n",
    "\n",
    "### Base Model\n",
    "\n",
    "The Base Model just consists of a basic feed forward network, no bells or whistles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_labels=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(num_features, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.05),\n",
    "            torch.nn.Linear(64, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.05),\n",
    "            torch.nn.Linear(128, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.05),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.05),\n",
    "            torch.nn.Linear(64, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def loss(self, output, y):\n",
    "        return {\n",
    "            \"mse\": F.mse_loss(output, y)\n",
    "        }\n",
    "\n",
    "    def get_output(self, output):\n",
    "        return output\n",
    "\n",
    "    def plot_data(self):\n",
    "        return\n",
    "    \n",
    "    def add_epoch_info(self, writer, epoch):\n",
    "        return\n",
    "\n",
    "    def name(self):\n",
    "        return \"Base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### \"Cheating\" Additive Model\n",
    "\n",
    "This model consists of a separate neural network for each invidiual feature (like NAM) and each pairwise combination of features (this would not scale for any reasonable number of features, but is a good test comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveModel(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_labels=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_features = num_features\n",
    "\n",
    "        self.first_order = torch.nn.ModuleList(\n",
    "            [torch.nn.Sequential(\n",
    "                torch.nn.Linear(1, 32),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Dropout(0.05),\n",
    "                torch.nn.Linear(32, 32),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Dropout(0.05),\n",
    "                torch.nn.Linear(32, 1)\n",
    "            ) for _ in range(num_features)]\n",
    "        )\n",
    "\n",
    "        combs = list(combinations(range(num_features), 2))\n",
    "\n",
    "        self.second_order = torch.nn.ModuleList(\n",
    "            [torch.nn.Sequential(\n",
    "                torch.nn.Linear(2, 32),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Dropout(0.05),\n",
    "                torch.nn.Linear(32, 32),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Dropout(0.05),\n",
    "                torch.nn.Linear(32, 1)\n",
    "            ) for _ in range(len(combs))]\n",
    "        )\n",
    "\n",
    "        self.final = torch.nn.Linear(len(combs) + num_features, num_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = torch.cat([self.first_order[i](x[:,i].unsqueeze(1)) for i in range(self.num_features)], dim=1)\n",
    "        s = torch.cat([self.second_order[i](x[:,p]) for i, p in enumerate(combinations(range(self.num_features), 2))], dim=1)\n",
    "        yhat = self.final(torch.cat([f, s], dim=1))\n",
    "\n",
    "        return yhat\n",
    "\n",
    "    def loss(self, output, y):\n",
    "        return {\n",
    "            \"mse\": F.mse_loss(output, y)\n",
    "        }\n",
    "\n",
    "    def get_output(self, output):\n",
    "        return output\n",
    "\n",
    "    def plot_data(self):\n",
    "        return\n",
    "    \n",
    "    def add_epoch_info(self, writer, epoch):\n",
    "        return\n",
    "\n",
    "    def name(self):\n",
    "        return \"Additive\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Attention Selection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        self.qkv_proj = nn.Linear(embed_dim, 3 * embed_dim * num_heads)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, num_features, embed_dim = x.size()\n",
    "        qkv = self.qkv_proj(x)\n",
    "        qkv = qkv.reshape(batch_size, num_features, self.num_heads, 3 * self.embed_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3) # (batch_size, num_heads, num_features, 3 * head_dim)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        \n",
    "        attn_weights = torch.matmul(q, k.transpose(-2, -1)) / (self.embed_dim ** 0.5)\n",
    "        \n",
    "        return attn_weights\n",
    "\n",
    "\n",
    "class GraphFragmentationLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GraphFragmentationLoss, self).__init__()\n",
    "    \n",
    "    def connectivity_loss(self, A):\n",
    "        # Sum of off-diagonal elements of adjacency matrix\n",
    "        return torch.sum(A) - torch.sum(torch.diag(A))\n",
    "    \n",
    "    def spectral_loss(self, L, k=1):\n",
    "        # Compute eigenvalues of Laplacian\n",
    "        eigenvalues = torch.linalg.eigvalsh(L)\n",
    "        # Sum of smallest non-zero eigenvalues\n",
    "        return torch.sum(eigenvalues[1:k+1])\n",
    "    \n",
    "    def sparsity_loss(self, A):\n",
    "        # L1 norm of adjacency matrix\n",
    "        return torch.norm(A, p=1)\n",
    "    \n",
    "    def forward(self, A):\n",
    "        # Compute degree matrix\n",
    "        D = torch.diag(A.sum(1))\n",
    "        # Compute Laplacian matrix\n",
    "        L = D - A\n",
    "        \n",
    "        # Loss components\n",
    "        conn_loss = self.connectivity_loss(A)\n",
    "        spec_loss = self.spectral_loss(L)\n",
    "        sparse_loss = self.sparsity_loss(A)\n",
    "        \n",
    "        return conn_loss, spec_loss, sparse_loss\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def laplacian_matrix(adj):\n",
    "    \"\"\"Compute the Laplacian matrix of the adjacency matrix.\"\"\"\n",
    "    degree = torch.diag(torch.sum(adj, dim=1))\n",
    "    laplacian = degree - adj\n",
    "    return laplacian\n",
    "\n",
    "def spectral_loss(laplacian, k=1):\n",
    "    \"\"\"Compute the spectral loss based on the smallest non-zero eigenvalues of the Laplacian.\"\"\"\n",
    "    eigenvalues, _ = torch.linalg.eigh(laplacian)\n",
    "    non_zero_eigenvalues = eigenvalues[eigenvalues > 1e-5]  # Filter out zero eigenvalues\n",
    "    smallest_non_zero_eigenvalues = non_zero_eigenvalues[:k]\n",
    "    return torch.sum(smallest_non_zero_eigenvalues)\n",
    "\n",
    "def spectral_loss_incl_zero(laplacian, k=1):\n",
    "    \"\"\"\n",
    "    Same as above, but just penalize the smallest k eigenvalues regardless of whether or not they are zero.\n",
    "    \"\"\"\n",
    "    eigenvalues, _ = torch.linalg.eigh(laplacian)  # TODO are these sorted?\n",
    "    print(\"Eigenvalues\", eigenvalues)\n",
    "    smallest_eigenvalues = eigenvalues[:k]\n",
    "    return torch.sum(smallest_eigenvalues)\n",
    "\n",
    "def edge_sparsity_loss(adj):\n",
    "    \"\"\"Compute the edge sparsity loss.\"\"\"\n",
    "    return torch.sum(adj)\n",
    "\n",
    "def connectivity_penalty(adj, k=3):\n",
    "    \"\"\"Compute the connectivity penalty using powers of the adjacency matrix.\"\"\"\n",
    "    adj_k = torch.matrix_power(adj, k)\n",
    "    return torch.trace(adj_k)\n",
    "\n",
    "def fragmentation_loss(adj, alpha=1.0, beta=1.0, gamma=1.0, k=1):\n",
    "    \"\"\"Compute the total fragmentation loss.\"\"\"\n",
    "    laplacian = laplacian_matrix(adj)\n",
    "    loss_spectral = spectral_loss(laplacian, k)\n",
    "    loss_sparsity = edge_sparsity_loss(adj)\n",
    "    loss_connectivity = connectivity_penalty(adj)\n",
    "\n",
    "    return loss_spectral, loss_sparsity, loss_connectivity\n",
    "\n",
    "class FeatureAttentionModel(nn.Module):\n",
    "    def __init__(self, num_features=4, embedding_dim=64, num_heads=8, include_spectral_loss=False, include_connectivity_loss=False, \n",
    "                 include_sparsity_loss=False, mse_weight=50, spectral_weight=10):\n",
    "        super(FeatureAttentionModel, self).__init__()\n",
    "\n",
    "        self.include_spectral_loss = include_spectral_loss\n",
    "        self.include_connectivity_loss = include_connectivity_loss\n",
    "        self.include_sparsity_loss = include_sparsity_loss\n",
    "        self.mse_weight = mse_weight\n",
    "        self.spectral_weight = spectral_weight\n",
    "        if include_spectral_loss or include_connectivity_loss or include_sparsity_loss:\n",
    "            self.include_graph_losses = True\n",
    "            self.GFL = GraphFragmentationLoss()\n",
    "        else:\n",
    "            self.include_graph_losses = False\n",
    "\n",
    "        self.num_features = num_features\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.embedding_layer = nn.Embedding(num_features, embedding_dim)\n",
    "        self.attention_layer = MultiHeadAttention(embedding_dim, num_heads)\n",
    "\n",
    "        self.linear1 = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(1, 16),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.05),\n",
    "                nn.Linear(16, 32),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.05),\n",
    "                nn.Linear(32, embedding_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.05)\n",
    "            ) for _ in range(num_features)\n",
    "        ])\n",
    "\n",
    "        self.linear2 = nn.ModuleList([\n",
    "            nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(num_features, 16),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(0.05),\n",
    "                    nn.Linear(16, 32),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(0.05),\n",
    "                    nn.Linear(32, embedding_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(0.05)\n",
    "                ) for _ in range(num_heads)\n",
    "            ]) for _ in range(num_features)\n",
    "        ])\n",
    "\n",
    "        self.proj = nn.Linear(num_features*embedding_dim + num_features * num_heads * embedding_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embs = self.embedding_layer(torch.tensor(range(self.num_features)).to(device))\n",
    "\n",
    "        mask = torch.eye(self.num_features).to(device)\n",
    "\n",
    "        attn = self.attention_layer(embs.unsqueeze(0))\n",
    "        # selection = sparsemax(attn - torch.diag(torch.inf * torch.ones(self.num_features)).to(device)) + mask\n",
    "        selection = sparsemax(attn) * (1 - mask) + mask\n",
    "\n",
    "        A = (selection[0].mT @ selection[0]).sum(dim=0)\n",
    "        degree = A.sum(dim=1)\n",
    "        D_inv_sqrt = torch.diag(torch.pow(degree, -0.5))\n",
    "        A = D_inv_sqrt @ A @ D_inv_sqrt\n",
    "        \n",
    "        selectionx = torch.einsum(\"ijk,bk->bjik\", selection[0], x)\n",
    "\n",
    "        h1 = torch.cat([l(x[:,i].unsqueeze(1)) for i, l in enumerate(self.linear1)], dim=1)\n",
    "\n",
    "        h2 = torch.cat([l(selectionx[:,i,j]) for i, l2 in enumerate(self.linear2) for j, l in enumerate(l2)], dim=1)\n",
    "\n",
    "        y = self.proj(torch.cat([h1, h2], dim=1))\n",
    "        \n",
    "        return embs, mask, attn, selection, selectionx, h1, h2, A, y\n",
    "\n",
    "    def loss(self, output, y):\n",
    "        A = output[-2]\n",
    "\n",
    "        mse_loss = F.mse_loss(output[-1], y)\n",
    "\n",
    "        losses = {\n",
    "            \"mse\": mse_loss\n",
    "        }\n",
    "\n",
    "        if self.include_graph_losses:\n",
    "            # loss_spectral, loss_sparsity, loss_connectivity = fragmentation_loss(A)\n",
    "            conn_loss, spec_loss, sparse_loss = self.GFL(A)\n",
    "\n",
    "            if self.include_spectral_loss:\n",
    "                losses[\"spectral\"] = self.spectral_weight * spec_loss\n",
    "\n",
    "            if self.include_sparsity_loss:\n",
    "                losses[\"sparsity\"] = sparse_loss\n",
    "\n",
    "            if self.include_connectivity_loss:\n",
    "                losses[\"connectivity\"] = conn_loss\n",
    "                \n",
    "            losses[\"mse\"] = self.mse_weight * losses[\"mse\"]\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def get_output(self, output):\n",
    "        return output[-1]\n",
    "\n",
    "    def plot_data(self, save=False, epoch=0, writer=None):\n",
    "        embs = self.embedding_layer(torch.tensor(range(self.num_features)).to(device))\n",
    "\n",
    "        mask = torch.eye(self.num_features).to(device)\n",
    "\n",
    "        attn = self.attention_layer(embs.unsqueeze(0))\n",
    "        # selection = sparsemax(attn - torch.diag(torch.inf * torch.ones(self.num_features)).to(device)) + mask\n",
    "        selection = sparsemax(attn) * (1 - mask) + mask\n",
    "\n",
    "        A = (selection[0].mT @ selection[0]).sum(dim=0)\n",
    "\n",
    "        A = A.cpu().detach()\n",
    "        selection = selection.mean(dim=(0,1)).cpu().detach()\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(20, 16))\n",
    "        heatmap = sns.heatmap(A, ax=ax, annot=True, cmap='coolwarm', fmt=\".2f\",\n",
    "            xticklabels=[f\"Feature {i}\" for i in range(self.num_features)],\n",
    "            yticklabels=[f\"Feature {i}\" for i in range(self.num_features)])\n",
    "        ax.set_title(\"Attention Matrix\")\n",
    "\n",
    "        if save:\n",
    "            heatmap.figure.savefig(\"A.png\")\n",
    "\n",
    "            # Load the heatmap image and convert it to a tensor\n",
    "            image = plt.imread(\"A.png\")\n",
    "            image_tensor = torch.tensor(image).permute(2, 0, 1).unsqueeze(0)\n",
    "    \n",
    "            writer.add_image(\"A\", image_tensor[0], epoch)\n",
    "\n",
    "            plt.close()\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(20, 16))\n",
    "        heatmap = sns.heatmap(selection, ax=ax, annot=True, cmap='coolwarm', fmt=\".2f\",\n",
    "            xticklabels=[f\"Feature {i}\" for i in range(self.num_features)],\n",
    "            yticklabels=[f\"Feature {i}\" for i in range(self.num_features)])\n",
    "        ax.set_title(\"Selection Heads (Aggregated)\")\n",
    "\n",
    "        if save:\n",
    "            heatmap.figure.savefig(\"selection.png\")\n",
    "    \n",
    "            # Load the heatmap image and convert it to a tensor\n",
    "            image = plt.imread(\"selection.png\")\n",
    "            image_tensor = torch.tensor(image).permute(2, 0, 1).unsqueeze(0)\n",
    "    \n",
    "            writer.add_image(\"selection\", image_tensor[0], epoch)\n",
    "\n",
    "            plt.close()\n",
    "\n",
    "        if not save:\n",
    "            plt.show()\n",
    "    \n",
    "    def add_epoch_info(self, writer, epoch):\n",
    "        self.plot_data(True, epoch, writer)\n",
    "\n",
    "    def name(self):\n",
    "        return \"FAM{}{}{}\".format(\"S\" if self.include_spectral_loss else \"\", \"E\" if self.include_sparsity_loss else \"\", \n",
    "                                  \"C\" if self.include_connectivity_loss else \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### KAN-based models\n",
    "\n",
    "Consider a model with a KAN-based encoder, which takes in features $x_1, \\dots, x_D$, and outputs hidden variables $z_1, \\dots, z_H$, ideally in a modular group structure that is clusterable (the network can be partitioned into multiple components, each with some inputs and outputs, such that the weight of edges between clusters is low). Then, the final layer produces output $y_j$, either by:\n",
    "\n",
    "1) Dot product (linear layer): \n",
    "$y_j = \\mathbf{w}^T \\mathbf{z} + b = b + \\sum_{i=1}^H w_i h_i$\n",
    "where $w_i, b$ are learnable parameters. NOT SUPPORTED YET.\n",
    "\n",
    "2) Generalized additive model (1-layer KAN):\n",
    "$y_j = b + \\sum_{i=1}^H \\phi_i(h_i; \\theta_i)$\n",
    "where the $\\phi_i$ are learnable activation functions (parameterized with B-splines). In this case, we include this as the last layer in the KAN, but do \"clusterability regularization\" excluding the last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_VARS = [f\"z{i}\" for i in range(12)]\n",
    "OUT_VARS = [\"y\"]\n",
    "PLOT_DIR = \".\"\n",
    "\n",
    "import kan\n",
    "class ModularKAN(nn.Module):\n",
    "    def __init__(self, num_features=4, num_outputs=1, num_layers=2, embedding_dim=4, final_layer=\"gam\",\n",
    "                 include_spectral_loss=False, include_connectivity_loss=False, include_sparsity_loss=False,\n",
    "                 mse_weight=50, spectral_weight=10,\n",
    "                 kan_grid=3, kan_grid_margin=1.0, kan_noise=0.3, kan_base_fun=\"silu\", kan_affine_trainable=True,\n",
    "                 kan_absolute_deviation=False, kan_flat_entropy=True, kan_update_grid_until=20):\n",
    "        \"\"\"\n",
    "        num_features: Number of input features.\n",
    "        num_outputs: Number of final outputs (1 if single-output)\n",
    "        num_layers: Number of layers EXCLUDING the final layer\n",
    "        embedding_dim: Number of neurons in hidden layers\n",
    "        final_layer: `gam` if final layer is a normal KAN layer, `linear` if we want a final dot-product\n",
    "        include_spectral_loss: True if you want to include spectral loss (penalizing the smallest eigenvalues ->\n",
    "            encouraging the graph to be more bottlenecked / easily clusterable)\n",
    "        include_connectivity_loss: True if you want to include connectivity penalty based on matrix powers\n",
    "        include_sparsity_loss: True to include an L1 penalty on the edge scores\n",
    "        \n",
    "\n",
    "        kan_update_grid_until: update grid every epoch until this number of epochs\n",
    "        \"\"\"\n",
    "        super(ModularKAN, self).__init__()\n",
    "\n",
    "        self.include_spectral_loss = include_spectral_loss\n",
    "        self.include_connectivity_loss = include_connectivity_loss\n",
    "        self.include_sparsity_loss = include_sparsity_loss\n",
    "        self.mse_weight = mse_weight\n",
    "        self.spectral_weight = spectral_weight\n",
    "        if include_spectral_loss or include_connectivity_loss or include_sparsity_loss:\n",
    "            self.include_graph_losses = True\n",
    "            self.GFL = GraphFragmentationLoss()\n",
    "        else:\n",
    "            self.include_graph_losses = False\n",
    "\n",
    "        self.num_features = num_features\n",
    "        self.num_outputs = num_outputs\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.final_layer = final_layer\n",
    "        self.kan_flat_entropy = kan_flat_entropy\n",
    "        self.update_grid_until = kan_update_grid_until\n",
    "\n",
    "        # Size of each layer: [inputs, hidden nodes, ..., outputs]\n",
    "        self.layer_sizes = [num_features] + [embedding_dim] * (num_layers) + [num_outputs]\n",
    "\n",
    "        # For the adjacency matrix, we have a node for each neuron (across all layers).\n",
    "        # Precompute which index each layer's neurons start from.\n",
    "        # For example, if layer 0 (input) neurons are 0-3, layer 1's neurons are 4-11, and layer 2 (output) neurons is 12, \n",
    "        # self.layer_starts would contain: [0, 4, 12, 13] - 13 is the end of the last layer (exclusive)\n",
    "        self.layer_starts = [0]\n",
    "        curr_idx = 0\n",
    "        for i, layer_size in enumerate(self.layer_sizes):\n",
    "            curr_idx += layer_size\n",
    "            self.layer_starts.append(curr_idx)\n",
    "        self.total_nodes = sum(self.layer_sizes)\n",
    "        assert self.total_nodes == curr_idx\n",
    "\n",
    "        # Initialize the KAN model\n",
    "        # TODO: Dot-product final layer is not supported yet\n",
    "        self.kan = kan.KAN(width=self.layer_sizes, grid=kan_grid, k=3, seed=torch.initial_seed(), device=device,\n",
    "                           noise_scale=kan_noise, base_fun=kan_base_fun, affine_trainable=kan_affine_trainable, grid_eps=1.0, \n",
    "                           grid_margin=kan_grid_margin, absolute_deviation=kan_absolute_deviation)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x  # Cache input\n",
    "        output = self.kan(x)\n",
    "\n",
    "        # KAN sparsity losses\n",
    "        # NOTE: the lamb values passed are completely unused, as we direclty obtain the individual loss components and weight them later.\n",
    "        # For default weights see https://github.com/KindXiaoming/pykan/blob/master/kan/MultKAN.py#L1411\n",
    "        kan_l1_loss, kan_entropy_loss, kan_coef_loss, kan_coefdiff_loss, kan_coefdiff2_loss, conn_cost = self.kan.reg(\n",
    "            reg_metric='edge_backward', lamb_l1=1., lamb_entropy=1., lamb_coef=1., lamb_coefdiff=1.,\n",
    "            return_indiv=True, flat_entropy=self.kan_flat_entropy)\n",
    "\n",
    "        # Count the number of edges per layer\n",
    "        A = torch.zeros((self.total_nodes, self.total_nodes))\n",
    "        for i, layer_edges in enumerate(self.kan.edge_scores):\n",
    "            # print(\"Layer\", i, layer_edges.shape)  # self.kan.edge_scores is a list. Each element is of shape [out_dim, in_dim]\n",
    "            input_nodes = slice(self.layer_starts[i], self.layer_starts[i+1])\n",
    "            output_nodes = slice(self.layer_starts[i+1], self.layer_starts[i+2])\n",
    "            A[input_nodes, output_nodes] = layer_edges.T\n",
    "            A[output_nodes, input_nodes] = layer_edges\n",
    "        self.A = A\n",
    "        self.conn_cost = conn_cost\n",
    "        return A, output\n",
    "\n",
    "    def loss(self, output, y):\n",
    "        A = output[-2]\n",
    "        \n",
    "        mse_loss = F.mse_loss(output[-1], y)\n",
    "\n",
    "        losses = {\n",
    "            \"mse\": mse_loss\n",
    "        }\n",
    "\n",
    "        if self.include_graph_losses:\n",
    "            # loss_spectral, loss_sparsity, loss_connectivity = fragmentation_loss(A)\n",
    "            conn_loss, spec_loss, sparse_loss = self.GFL(A)\n",
    "\n",
    "            if self.include_spectral_loss:\n",
    "                losses[\"spectral\"] = self.spectral_weight * spec_loss\n",
    "\n",
    "            if self.include_sparsity_loss:\n",
    "                losses[\"sparsity\"] = sparse_loss\n",
    "\n",
    "            if self.include_connectivity_loss:\n",
    "                losses[\"connectivity\"] = conn_loss\n",
    "\n",
    "            if self.include_\n",
    "            losses[\"mse\"] = self.mse_weight * losses[\"mse\"]\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def get_output(self, output):\n",
    "        return output[-1]\n",
    "\n",
    "    def plot_data(self, save=False, epoch=0, writer=None):\n",
    "\n",
    "        # Plot the KAN\n",
    "        # Produce edge/node importance scores\n",
    "        self.kan.attribute()\n",
    "        self.kan.node_attribute()\n",
    "\n",
    "        # Plot the unpruned model\n",
    "        self.kan.plot(folder=os.path.join(PLOT_DIR, \"splines\"), in_vars=IN_VARS, out_vars=OUT_VARS, scale=5, varscale=0.13)\n",
    "        if save:\n",
    "            plt.savefig(os.path.join(PLOT_DIR, f\"epoch{epoch}_kan_plot.png\"))\n",
    "            plt.close()\n",
    "\n",
    "        # Plot the adjacency matrix\n",
    "        A = self.A.cpu().detach()\n",
    "        # selection = selection.mean(dim=(0,1)).cpu().detach()\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(20, 16))\n",
    "        heatmap = sns.heatmap(A, ax=ax, annot=True, cmap='coolwarm', fmt=\".2f\",\n",
    "            xticklabels=[f\"Feature {i}\" for i in range(self.num_features)],\n",
    "            yticklabels=[f\"Feature {i}\" for i in range(self.num_features)])\n",
    "        ax.set_title(\"Attention Matrix\")\n",
    "\n",
    "        if save:\n",
    "            heatmap.figure.savefig(\"A.png\")\n",
    "\n",
    "            # Load the heatmap image and convert it to a tensor\n",
    "            image = plt.imread(\"A.png\")\n",
    "            image_tensor = torch.tensor(image).permute(2, 0, 1).unsqueeze(0)\n",
    "    \n",
    "            writer.add_image(\"A\", image_tensor[0], epoch)\n",
    "\n",
    "            plt.close()\n",
    "\n",
    "    \n",
    "    def add_epoch_info(self, writer, epoch):\n",
    "        \"\"\"\n",
    "        joshuafan: I'm overloading this method so that it also performs KAN\n",
    "        operations that should be run infrequently, such as (1) updating the spline\n",
    "        grids, and (2) swapping to reduce connection cost (encourage modularity)\n",
    "        \"\"\"\n",
    "        print(\"Add epoch info!\")\n",
    "        # Update grid\n",
    "        if epoch < self.update_grid_until:\n",
    "            with torch.no_grad():\n",
    "                self.kan.update_grid(self.x)\n",
    "\n",
    "        # Swap to reduce connection cost\n",
    "        self.kan.auto_swap()\n",
    "\n",
    "        # Create plots\n",
    "        self.plot_data(True, epoch, writer)\n",
    "\n",
    "        \n",
    "\n",
    "    def name(self):\n",
    "        return \"KAN{}{}{}\".format(\"S\" if self.include_spectral_loss else \"\", \"E\" if self.include_sparsity_loss else \"\", \n",
    "                                  \"C\" if self.include_connectivity_loss else \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Next we train the different models we're working with and plotting the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_results(model, losses, metrics, data_generator):\n",
    "    # Generate new data\n",
    "    xy = torch.rand((15000, 2)).to(device)\n",
    "    \n",
    "    z = xy_to_z(xy[:,0].unsqueeze(1), xy[:,1].unsqueeze(1))\n",
    "\n",
    "    labels = data_generator.calculate(z)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        lhat = model.get_output(model(z))\n",
    "\n",
    "    plot_data(xy, labels, title=\"True Data\")\n",
    "    plot_data(xy, lhat, title=\"{} Predictions\".format(model.name()))\n",
    "    plot_data(xy, torch.log(torch.abs(labels.unsqueeze(1) - lhat)), title=\"{} Predictions vs. True Data\".format(model.name()))\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(20, 4))\n",
    "\n",
    "    # NOTE: Start at 1 because the initial loss is very high\n",
    "    axs[0].plot(range(1, len(losses['train'])), losses[\"train\"][1:], linewidth=3)\n",
    "    axs[0].set_title(\"Training Loss\")\n",
    "    axs[0].set_xlabel(\"Epoch\")\n",
    "    axs[0].set_ylabel(\"Loss\")\n",
    "\n",
    "    axs[1].plot(range(1, len(losses['valid'])), losses[\"valid\"][1:], linewidth=3)\n",
    "    axs[1].set_title(\"Validation Loss\")\n",
    "    axs[1].set_xlabel(\"Epoch\")\n",
    "    axs[1].set_ylabel(\"Loss\")\n",
    "\n",
    "    axs[2].plot(range(1, len(losses['test'])), losses[\"test\"][1:], linewidth=3)\n",
    "    axs[2].set_title(\"Test Loss\")\n",
    "    axs[2].set_xlabel(\"Epoch\")\n",
    "    axs[2].set_ylabel(\"Loss\")\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(20, 4))\n",
    "\n",
    "    # NOTE: Start at 1 because the MSE is very high\n",
    "    axs[0].plot(range(1, len(metrics['train'])), metrics[\"train\"][1:], linewidth=3)\n",
    "    axs[0].set_title(\"Training MSE\")\n",
    "    axs[0].set_xlabel(\"Epoch\")\n",
    "    axs[0].set_ylabel(\"MSE\")\n",
    "\n",
    "    axs[1].plot(range(1, len(metrics['valid'])), metrics[\"valid\"][1:], linewidth=3)\n",
    "    axs[1].set_title(\"Validation MSE\")\n",
    "    axs[1].set_xlabel(\"Epoch\")\n",
    "    axs[1].set_ylabel(\"MSE\")\n",
    "\n",
    "    axs[2].plot(range(1, len(metrics['test'])), metrics[\"test\"][1:], linewidth=3)\n",
    "    axs[2].set_title(\"Test MSE\")\n",
    "    axs[2].set_xlabel(\"Epoch\")\n",
    "    axs[2].set_ylabel(\"MSE\")\n",
    "\n",
    "    model.plot_data()\n",
    "\n",
    "def model_summary(model):\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Trainable parameters for {model.name()}: {trainable_params}\")\n",
    "\n",
    "def train(model, num_epochs, train_loader, valid_loader, test_loader, data_generator, run=None):\n",
    "    optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "    name = model.name()\n",
    "\n",
    "    if run is not None:\n",
    "        name = \"{}_{}\".format(name, run)\n",
    "\n",
    "    # Create a SummaryWriter instance\n",
    "    writer = SummaryWriter(log_dir='logs/{}'.format(name))\n",
    "\n",
    "    print(\"Training {}\".format(name))\n",
    "    model_summary(model)\n",
    "\n",
    "    best_model = model.parameters()\n",
    "    best_mse = None\n",
    "\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "    loss_epochs = {\n",
    "        \"train\": [],\n",
    "        \"valid\": [],\n",
    "        \"test\": []\n",
    "    }\n",
    "\n",
    "    metric_epochs = {\n",
    "        \"train\": [],\n",
    "        \"valid\": [],\n",
    "        \"test\": []\n",
    "    }\n",
    "\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(num_epochs), leave=True):\n",
    "        all_probs = []\n",
    "        all_labels = []\n",
    "\n",
    "        all_losses = {}\n",
    "        \n",
    "        for x, y in train_loader:\n",
    "            output = model(x)\n",
    "\n",
    "            losses = model.loss(output, y)\n",
    "\n",
    "            loss = 0\n",
    "            for v, l in losses.items():\n",
    "                loss += l\n",
    "                if v not in all_losses:\n",
    "                    all_losses[v] = []\n",
    "                all_losses[v].append(l.detach())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            all_probs.append(model.get_output(output).detach())\n",
    "            all_labels.append(y.detach())\n",
    "\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "        all_labels = torch.cat(all_labels, dim=0)\n",
    "        train_losses = {k: torch.cat([c.unsqueeze(0) for c in l], dim=0).mean() for k, l in all_losses.items()}\n",
    "\n",
    "        train_mse = mean_squared_error(all_probs.flatten(), all_labels.flatten())\n",
    "\n",
    "        loss = 0\n",
    "        for l in train_losses.values():\n",
    "            loss += l.cpu().numpy()\n",
    "        \n",
    "        loss_epochs[\"train\"].append(loss)\n",
    "        metric_epochs[\"train\"].append(train_mse.cpu().detach().numpy())\n",
    "\n",
    "        model.eval()\n",
    "        \n",
    "        all_probs = []\n",
    "        all_labels = []\n",
    "\n",
    "        all_losses = {}\n",
    "        \n",
    "        for x, y in valid_loader:\n",
    "            with torch.no_grad():\n",
    "                output = model(x)\n",
    "                \n",
    "                losses = model.loss(output, y)\n",
    "                loss = 0\n",
    "                for v, l in losses.items():\n",
    "                    loss += l\n",
    "                    if v not in all_losses:\n",
    "                        all_losses[v] = []\n",
    "                    all_losses[v].append(l.detach())\n",
    "\n",
    "            all_probs.append(model.get_output(output).detach())\n",
    "            all_labels.append(y.detach())\n",
    "\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "        all_labels = torch.cat(all_labels, dim=0)\n",
    "        valid_losses = {k: torch.cat([c.unsqueeze(0) for c in l], dim=0).mean() for k, l in all_losses.items()}\n",
    "\n",
    "        valid_mse = mean_squared_error(all_probs.flatten(), all_labels.flatten())\n",
    "\n",
    "        loss = 0\n",
    "        for l in valid_losses.values():\n",
    "            loss += l.cpu().numpy()\n",
    "        \n",
    "        loss_epochs[\"valid\"].append(loss)\n",
    "        metric_epochs[\"valid\"].append(valid_mse.cpu().detach().numpy())\n",
    "\n",
    "        if best_mse is None or valid_mse < best_mse:\n",
    "            best_mse = valid_mse\n",
    "            torch.save(model.state_dict(), \"models/{}.pt\".format(name))\n",
    "            best_model = model.state_dict()\n",
    "\n",
    "        all_probs = []\n",
    "        all_labels = []\n",
    "\n",
    "        all_losses = {}\n",
    "        \n",
    "        for x, y in test_loader:\n",
    "            with torch.no_grad():\n",
    "                output = model(x)\n",
    "                \n",
    "                losses = model.loss(output, y)\n",
    "                loss = 0\n",
    "                for v, l in losses.items():\n",
    "                    loss += l\n",
    "                    if v not in all_losses:\n",
    "                        all_losses[v] = []\n",
    "                    all_losses[v].append(l.detach())\n",
    "\n",
    "            all_probs.append(model.get_output(output).detach())\n",
    "            all_labels.append(y.detach())\n",
    "\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "        all_labels = torch.cat(all_labels, dim=0)\n",
    "        test_losses = {k: torch.cat([c.unsqueeze(0) for c in l], dim=0).mean() for k, l in all_losses.items()}\n",
    "\n",
    "        test_mse = mean_squared_error(all_probs.flatten(), all_labels.flatten())\n",
    "\n",
    "        loss = 0\n",
    "        for l in test_losses.values():\n",
    "            loss += l.cpu().numpy()\n",
    "        \n",
    "        loss_epochs[\"test\"].append(loss)\n",
    "        metric_epochs[\"test\"].append(test_mse.cpu().detach().numpy())\n",
    "\n",
    "        writer.add_scalar(\"MSE/train\", train_mse, epoch)\n",
    "        writer.add_scalar(\"MSE/valid\", valid_mse, epoch)\n",
    "        writer.add_scalar(\"MSE/test\", test_mse, epoch)\n",
    "\n",
    "        for k, loss in train_losses.items():\n",
    "            writer.add_scalar(\"Loss/{}/train\".format(k), loss, epoch)\n",
    "\n",
    "        for k, loss in valid_losses.items():\n",
    "            writer.add_scalar(\"Loss/{}/valid\".format(k), loss, epoch)\n",
    "\n",
    "        for k, loss in test_losses.items():\n",
    "            writer.add_scalar(\"Loss/{}/test\".format(k), loss, epoch)\n",
    "\n",
    "        model.add_epoch_info(writer, epoch)\n",
    "        \n",
    "        if epoch % 10 == 0 or epoch == num_epochs - 1:\n",
    "            print(\"{} TRAINING MSE: {:.3f} VALID MSE: {:.3f} TEST MSE: {:.3f}\".format(epoch, train_mse, valid_mse, test_mse))\n",
    "            print(\"\\tTRAINING LOSSES: {}\".format(\" \".join([\"{}: {:.3f}\".format(k, l) for k, l in train_losses.items()])))\n",
    "            print(\"\\tVALIDATION LOSSES: {}\".format(\" \".join([\"{}: {:.3f}\".format(k, l) for k, l in valid_losses.items()])))\n",
    "            print(\"\\tTEST LOSSES: {}\".format(\" \".join([\"{}: {:.3f}\".format(k, l) for k, l in test_losses.items()])))\n",
    "\n",
    "    model.load_state_dict(best_model)\n",
    "\n",
    "    print(\"Plotting Results\")\n",
    "\n",
    "    plot_model_results(model, loss_epochs, metric_epochs, data_generator)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "#### Base Model\n",
    "\n",
    "Base Model run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaseModel(num_features).to(device)\n",
    "\n",
    "train(model, num_epochs, train_loader, valid_loader, test_loader, data_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "#### \"Cheating\" Additive Model\n",
    "\n",
    "Additive Model run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AdditiveModel(num_features).to(device)\n",
    "\n",
    "train(model, num_epochs, train_loader, valid_loader, test_loader, data_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "#### KAN-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 8\n",
    "\n",
    "model = ModularKAN(num_features=num_features, num_outputs=1, num_layers=2, embedding_dim=embedding_dim,\n",
    "                   include_spectral_loss=True, include_connectivity_loss=True, include_sparsity_loss=True).to(device)\n",
    "\n",
    "train(model, num_epochs, train_loader, valid_loader, test_loader, data_generator, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "#### Feature Attention Model\n",
    "\n",
    "Feature Attention Model with just MSE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 32\n",
    "num_heads = 6\n",
    "\n",
    "model = FeatureAttentionModel(num_features, embedding_dim, num_heads).to(device)\n",
    "\n",
    "train(model, num_epochs, train_loader, valid_loader, test_loader, data_generator, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "Feature Attention Model with graph-based losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 32\n",
    "num_heads = 6\n",
    "\n",
    "model = FeatureAttentionModel(num_features, embedding_dim, num_heads, True, False, True, 500).to(device)\n",
    "\n",
    "train(model, num_epochs, train_loader, valid_loader, test_loader, data_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
